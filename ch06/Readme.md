## K-means法

**K-means法**は教師なし機械学習における代表的なクラスタリングアルゴリズムで，データを類似した特徴を持つグループ（クラスタ）に分割する手法である．このアルゴリズムの目標は，データ空間をK個のクラスタに分割し，各データポイントを最も近い重心（セントロイド）に所属させることで，クラスタ内の分散を最小化することである．

### 動作原理

アルゴリズムは期待値最大化（EM）アプローチに従い，以下のステップで繰り返し実行される：

1. **初期化**：K個のクラスタ重心をランダムに配置する
2. **割り当て（Eステップ）**：各データポイントを最も近い重心に基づいてクラスタに割り当てる
3. **更新（Mステップ）**：各クラスタの重心を，現在のクラスタに所属するデータポイントの平均値として再計算する
4. **収束判定**：重心の移動が停止するか，クラスタ割り当てが変化しなくなるまで繰り返す

このプロセスはWCSS（Within Cluster Sum of Squares）を最小化する方向に進行し，必ず収束するが，必ずしも全局的最適解に到達するとは限らない．

### 主要特徴

**利点**：
- 実装が比較的簡単で大規模データセットにもスケールする
- 計算コストが低く，効率的に実行できる

**欠点**：
- クラスタ数Kを事前に指定する必要があり，初期値に依存する
- 円形または球形のクラスタを前提としており，外れ値の影響を受けやすい
- 次元の呪いにより，高次元データでは効果が低下する

クラスタ数の選択には，シルエット法やエルボー法などの技術が用いられる．

## Word2Vec（Word法）

**Word2Vec**は，自然言語処理における単語埋め込み（Word Embedding）技術であり，単語を高次元の連続ベクトル空間にマッピングする手法である．この手法は，意味的に類似した単語がベクトル空間上でも近い位置に配置されるという原理に基づいており，Googleの研究者たちによって開発された．

### アーキテクチャ

Word2Vecは2種類の主要なモデルアーキテクチャを提供する：

**1. CBOW（Continuous Bag of Words）**
- コンテキスト単語（周辺の単語）から，対象の単語を予測するモデル
- 特定のウィンドウサイズ内の文脈を入力として，中央の単語を出力する

**2. Skip-gram**
- 対象の単語から，周辺のコンテキスト単語を予測するモデル
- 入力層に現在の単語を与え，出力層にコンテキスト単語を生成する

### トレーニングプロセス

モデルの学習は以下のステップで行われる：
- テキストデータを読み込み，前処理を実施
- 対象単語とコンテキスト単語のマッピングを作成
- ワンホットエンコーディングを生成
- ニューラルネットワークをトレーニング
- 隠れ層の重みを単語埋め込みベクトルとして抽出

得られたベクトル表現は，単語の意味的な類似性や関係性を計算するのに利用可能であり，類義語検出やアナロジー推論などのタスクで広く使用されている．

## t-SNE

**t-SNE（t-Distributed Stochastic Neighbor Embedding）**は，高次元データを2次元または3次元のマップ上に可視化するための次元削減技術である．高次元空間におけるデータ点間の類似性を低次元空間に保持しながら，各データ点の位置を学習する統計的手法である．

### アルゴリズムの2段階

**第一段階**：高次元空間におけるデータ点のペアに対して，類似したデータ点に高い確率，異なるデータ点に低い確率が割り当てられる確率分布を構築する．

**第二段階**：低次元マップ上のデータ点に対して同様の確率分布を定義し，高次元空間と低次元空間の2つの分布間のKullback-Leibler（KL）ダイバージェンスを最小化するように，マップ上のデータ点の位置を最適化する．

### 主な特性

- **可視化特化**：高次元データの構造やパターンを視覚的に理解するのに特に適している
- **確率的アプローチ**：データ点間の類似性を確率分布でモデル化し，局所的な構造を保持する
- **最適化**：勾配降下法を用いてコスト関数（KLダイバージェンス）を最小化する

t-SNEはデータ探索や理解の段階で広く利用され，特に画像データやテキストデータの可視化において強力なツールとして機能する．
