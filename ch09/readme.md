# 9章 思考
## 80
- サブワード分割は低頻度後や未知語を効率的に扱って語彙サイズを抑え，モデル泥濃を向上させるためのもの
- 最近は形態素解析ではなくSentencePieceで生の文からサブワードを抽出するのが標準．これは形態素解析のOOV問題に対処できるが，SentencePieceは学習するために初期学習データに依存する？
- ##は前トークンに依存の意味
- BPEはバイト対符号化のことで，サブワード分割に良く用いられる．文単位のBPEではSentencePieceが良く用いられる．
- BERTではWordPieceを採用．
## 81
- logitsは確率ではなく厳密にはまだスコアの段階
- BERTでのシーケンスとはトークンIDの数列のことで，文章全体の表現
## 82
- logitsはsoftmax関数で確率化
- dimはPyTouchテンソル演算でどの次元に対しての処理なのかを指すパラメータ(語彙次元が[1,約3万]で配列index0初めから見ると約3万に対する処理)
- 尤度(Likelihood)は，機械学習でモデルが観測データにどれだけ適合するか
### softmax関数
$$
\text{softmax}(Z)_{i,j} = \frac{e^{Z_{i,j}}}{\sum_{k=1}^{V} e^{Z_{i,k}}}
$$
## 83
- CLSトークンとは，BERTでの特殊トークンで入力シーケンスの先頭に置かれる．文中のすべての単語から情報を集めるように訓練されていて，文全体の要約として使用される(文埋め込み)
- funとexcitement(ポジティブ)やcrapとrubbish(ネガティブ)で高い類似度になっているが，文ベクトル(BERTのCLS埋め込み)により大体0.9を超える結果になった
## 84
- CLSではなく文全体の平均をとることで，ポジティブとネガティブの微量な差がoutに反映されている
## 85
- tokenizer.tokenize()は，純粋にトークンの文字列リストに変換する
- pandasのapply(数万行のデータを自動で1行ずつ高速に処理するメソッド)でベクトル化処理し，メモリ効率よくJITコンパイルで高速に動いてる
## 86
- GPUの行列計算のために各行の長さをそろえる作業をしており，[PAD]という空トークンを付け足して最長文にそろえる
- Input IDsはトークンの辞書番号(101=CLS,0=PAD)
- Attention Maskはモデルに対しての意味のある場所はどこかのヒント
## 87